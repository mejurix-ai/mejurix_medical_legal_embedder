---
language: en
tags:
- medical
- legal
- embedding
- ner
- clinical
- bert
- transformers
license: mit
datasets:
- custom
metrics:
- cosine similarity
library_name: transformers
pipeline_tag: feature-extraction
---

# Mejurix Medical-Legal Embedding Model

This model is a fine-tuned Transformer (BERT-based) that generates high-quality embeddings for documents in medical and legal domains, with a focus on capturing the semantic relationships between medical and legal concepts. The model leverages NER (Named Entity Recognition) to better understand domain-specific entities and their relationships.
Explore the model on Hugging Face:
ğŸ‘‰ https://huggingface.co/mejurix/medical-legal-embedder

## Model Description

### Model Architecture

- **Base Architecture**: BERT (Bidirectional Encoder Representations from Transformers)
- **Base Model**: medicalai/ClinicalBERT
- **Modifications**:
  - Custom embedding projection layer (768 â†’ 256 dimensions)
  - NER-enhanced attention mechanism
  - Domain-specific fine-tuning

### Key Features

- **Domain-Specific Embeddings**: Optimized for medical and legal text analysis
- **NER-Enhanced Understanding**: Utilizes named entity recognition to improve context awareness
- **Reduced Dimensionality**: 256-dimensional embeddings balance expressiveness and efficiency
- **Cross-Domain Connections**: Effectively captures relationships between medical findings and legal implications
- **Transformer-Based**: Leverages bidirectional attention mechanisms for better context understanding

## Performance Comparison

Our model outperforms other similar domain-specific models:

| Model          |   Avg Similarity | #Params   | Notes                  |
|:---------------|-----------------:|:----------|:-----------------------|
| **Mejurix (ours)** |      **0.9859** | 110M      | Medical-legal + NER FT |
| ClinicalBERT   |         0.9719 | 110M      | No NER, no fine-tuning |
| BioBERT        |         0.9640 | 110M      | Domain medical only    |
| LegalBERT      |         0.9508 | 110M      | Domain legal only      |

The Mejurix model shows superior performance across all relationship types, particularly in cross-domain relationships between medical and legal concepts.

### Detailed Relationship-Type Comparison

Our model demonstrates consistently higher similarity scores across all relationship types compared to other domain-specific models:

| Relationship Type | Mejurix | ClinicalBERT | BioBERT | LegalBERT |
|------------------|---------|--------------|---------|-----------|
| DISEASE_MEDICATION | 0.9966 | 0.9921 | 0.9841 | 0.8514 |
| SEVERITY_PROGNOSIS | 1.0000 | 1.0000 | 1.0000 | 0.8381 |
| SEVERITY_COMPENSATION | 0.9997 | 0.9606 | 0.9713 | 0.8348 |
| DISEASE_TREATMENT | 0.9980 | 0.9778 | 0.9645 | 0.8359 |
| DIAGNOSIS_TREATMENT | 0.9995 | 0.9710 | 0.9703 | 0.8222 |
| LEGAL_SIMILAR_MEDICAL_DIFFERENT | 0.9899 | 0.9699 | 0.9792 | 0.8236 |
| TREATMENT_OUTCOME | 0.9941 | 0.9668 | 0.9745 | 0.8103 |
| OUTCOME_SETTLEMENT | 0.9847 | 0.9631 | 0.9534 | 0.7951 |
| MEDICAL_SIMILAR_LEGAL_DIFFERENT | 0.9936 | 0.9434 | 0.9414 | 0.7812 |
| SYMPTOM_DISEASE | 0.9934 | 0.9690 | 0.9766 | 0.8500 |

The Mejurix model particularly excels in cross-domain relationships such as MEDICAL_SIMILAR_LEGAL_DIFFERENT (0.9936) and SEVERITY_COMPENSATION (0.9997), showing significant improvement over other models in these complex relationship types.

![Model Comparison by Relationship Type](./model_relationship_comparison.png)

## How to Use This Model

This model is directly available on the Hugging Face Hub and can be used with the Transformers library for feature extraction, sentence embeddings, and similarity calculations.

### Basic Usage with Transformers

```python
import torch
from transformers import AutoModel, AutoTokenizer

# Load model and tokenizer
model_name = "mejurix/medical-legal-embedder"  # The model's actual path on Hugging Face Hub
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Generate embeddings for a single text
text = "The patient was diagnosed with L3 vertebral fracture, and a compensation claim is in progress."
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)

with torch.no_grad():
    outputs = model(**inputs)

# Use the [CLS] token embedding for sentence representation
embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token
print(f"Embedding shape: {embeddings.shape}")  # Should be [1, 256]
```

### Using the Model for Similarity Calculation

```python
import torch
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer

# Load model and tokenizer
model_name = "mejurix/medical-legal-embedder"  # The model's actual path on Hugging Face Hub
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding

def compute_similarity(text1, text2):
    emb1 = get_embedding(text1)
    emb2 = get_embedding(text2)
    return F.cosine_similarity(emb1, emb2).item()

# Example
text1 = "Diagnosed with L3 spinal fracture."
text2 = "Compensation is needed for lumbar injury."
similarity = compute_similarity(text1, text2)
print(f"Similarity: {similarity:.4f}")
```

### Using with Hugging Face Pipelines

```python
from transformers import pipeline

# Create a feature-extraction pipeline
extractor = pipeline(
    "feature-extraction",
    model="mejurix/medical-legal-embedder",  # The model's actual path on Hugging Face Hub
    tokenizer="mejurix/medical-legal-embedder"
)

# Extract features
text = "The patient requires physical therapy following spinal surgery."
features = extractor(text)

# The output is a nested list with shape [1, sequence_length, hidden_size]
```

## Intended Uses & Limitations

### Intended Uses

- Medical-legal document similarity analysis
- Case relevance assessment
- Document clustering and organization
- Information retrieval in medical and legal domains
- Cross-referencing medical records with legal precedents
- Zero-shot text classification with custom categories

### Limitations

- Limited understanding of negations (current similarity: 0.7791)
- Temporal context differentiation needs improvement
- May not fully distinguish severity levels in medical conditions
- Maximum context length of 512 tokens (inherited from BERT architecture)

## Training and Evaluation

### Training

The model was fine-tuned on a specialized dataset containing medical-legal document pairs with various relationship types (disease-treatment, severity-compensation, etc.). Training employed triplet loss with hard negative mining.

**Training Configuration:**
- Base model: medicalai/ClinicalBERT
- Embedding dimension reduction: 768 â†’ 256
- Dropout: 0.5
- Learning rate: 1e-5
- Batch size: 16
- Weight decay: 0.1
- Triplet margin: 2.0
- Epochs: 15

## Citation

If you use this model in academic research, please cite:

```
@software{mejurix_medicallegal_embedder,
  author = {Mejurix},
  title = {Mejurix Medical-Legal Embedding Model},
  year = {2025},
  version = {0.1.0},
  url = {https://huggingface.co/mejurix/medical-legal-embedder}
}
```

## License

This project is distributed under the MIT License. See the LICENSE file for details.

---

# í•œêµ­ì–´ ë¬¸ì„œ / Korean Documentation

# Mejurix ì˜ë£Œ-ë²•ë¥  ì„ë² ë”© ëª¨ë¸

ë³¸ ëª¨ë¸ì€ ì˜ë£Œ ë° ë²•ë¥  ë„ë©”ì¸ì˜ í…ìŠ¤íŠ¸ì— íŠ¹í™”ëœ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ë¯¸ì„¸ ì¡°ì •ëœ íŠ¸ëœìŠ¤í¬ë¨¸(BERT ê¸°ë°˜) ëª¨ë¸ì…ë‹ˆë‹¤. ì˜ë£Œ ë° ë²•ë¥  ê°œë… ê°„ì˜ ì˜ë¯¸ë¡ ì  ê´€ê³„ë¥¼ í¬ì°©í•˜ëŠ” ë° ì¤‘ì ì„ ë‘ê³  ìˆìœ¼ë©°, ê°œì²´ëª… ì¸ì‹(NER)ì„ í™œìš©í•˜ì—¬ ë„ë©”ì¸ íŠ¹í™” ì—”í‹°í‹°ì™€ ê·¸ ê´€ê³„ë¥¼ ë” ì˜ ì´í•´í•©ë‹ˆë‹¤.

## ëª¨ë¸ ì„¤ëª…

### ëª¨ë¸ ì•„í‚¤í…ì²˜

- **ê¸°ë³¸ ì•„í‚¤í…ì²˜**: BERT (Bidirectional Encoder Representations from Transformers)
- **ê¸°ë°˜ ëª¨ë¸**: medicalai/ClinicalBERT
- **ì£¼ìš” ìˆ˜ì •ì‚¬í•­**:
  - ì‚¬ìš©ì ì •ì˜ ì„ë² ë”© íˆ¬ì˜ ë ˆì´ì–´ (768 â†’ 256 ì°¨ì›)
  - NER ê°•í™” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
  - ë„ë©”ì¸ íŠ¹í™” ë¯¸ì„¸ ì¡°ì •

### ì£¼ìš” íŠ¹ì§•

- **ë„ë©”ì¸ íŠ¹í™” ì„ë² ë”©**: ì˜ë£Œ ë° ë²•ë¥  í…ìŠ¤íŠ¸ ë¶„ì„ì— ìµœì í™”
- **NER ê°•í™” ì´í•´**: ê°œì²´ëª… ì¸ì‹ì„ í™œìš©í•˜ì—¬ ë§¥ë½ ì¸ì‹ ê°œì„ 
- **ì°¨ì› ì¶•ì†Œ**: 256ì°¨ì› ì„ë² ë”©ìœ¼ë¡œ í‘œí˜„ë ¥ê³¼ íš¨ìœ¨ì„±ì˜ ê· í˜• ìœ ì§€
- **í¬ë¡œìŠ¤ ë„ë©”ì¸ ì—°ê²°**: ì˜ë£Œ ì†Œê²¬ê³¼ ë²•ë¥ ì  í•¨ì˜ ê°„ì˜ ê´€ê³„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©
- **íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜**: ì–‘ë°©í–¥ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•˜ì—¬ ë§¥ë½ ì´í•´ í–¥ìƒ

## ì„±ëŠ¥ ë¹„êµ

ë³¸ ëª¨ë¸ì€ ìœ ì‚¬í•œ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤:

| ëª¨ë¸          | í‰ê·  ìœ ì‚¬ë„ | íŒŒë¼ë¯¸í„° ìˆ˜ | ë¹„ê³                     |
|:--------------|------------:|:------------|:------------------------|
| **Mejurix (ë³¸ ëª¨ë¸)** | **0.9859** | 110M      | ì˜ë£Œ-ë²•ë¥  + NER ë¯¸ì„¸ ì¡°ì • |
| ClinicalBERT  | 0.9719      | 110M      | NER ì—†ìŒ, ë¯¸ì„¸ ì¡°ì • ì—†ìŒ |
| BioBERT       | 0.9640      | 110M      | ì˜ë£Œ ë„ë©”ì¸ë§Œ íŠ¹í™”      |
| LegalBERT     | 0.9508      | 110M      | ë²•ë¥  ë„ë©”ì¸ë§Œ íŠ¹í™”      |

Mejurix ëª¨ë¸ì€ ëª¨ë“  ê´€ê³„ ìœ í˜•ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, íŠ¹íˆ ì˜ë£Œì™€ ë²•ë¥  ê°œë… ê°„ì˜ í¬ë¡œìŠ¤ ë„ë©”ì¸ ê´€ê³„ì—ì„œ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤.

### ê´€ê³„ ìœ í˜•ë³„ ìƒì„¸ ë¹„êµ

ë³¸ ëª¨ë¸ì€ ë‹¤ë¥¸ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ ëª¨ë“  ê´€ê³„ ìœ í˜•ì—ì„œ ì¼ê´€ë˜ê²Œ ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:

| ê´€ê³„ ìœ í˜• | Mejurix | ClinicalBERT | BioBERT | LegalBERT |
|------------------|---------|--------------|---------|-----------|
| DISEASE_MEDICATION (ì§ˆë³‘-ì•½ë¬¼) | 0.9966 | 0.9921 | 0.9841 | 0.8514 |
| SEVERITY_PROGNOSIS (ì¤‘ì¦ë„-ì˜ˆí›„) | 1.0000 | 1.0000 | 1.0000 | 0.8381 |
| SEVERITY_COMPENSATION (ì¤‘ì¦ë„-ë³´ìƒ) | 0.9997 | 0.9606 | 0.9713 | 0.8348 |
| DISEASE_TREATMENT (ì§ˆë³‘-ì¹˜ë£Œ) | 0.9980 | 0.9778 | 0.9645 | 0.8359 |
| DIAGNOSIS_TREATMENT (ì§„ë‹¨-ì¹˜ë£Œ) | 0.9995 | 0.9710 | 0.9703 | 0.8222 |
| LEGAL_SIMILAR_MEDICAL_DIFFERENT (ë²•ì  ìœ ì‚¬-ì˜í•™ì  ìƒì´) | 0.9899 | 0.9699 | 0.9792 | 0.8236 |
| TREATMENT_OUTCOME (ì¹˜ë£Œ-ê²°ê³¼) | 0.9941 | 0.9668 | 0.9745 | 0.8103 |
| OUTCOME_SETTLEMENT (ê²°ê³¼-í•©ì˜) | 0.9847 | 0.9631 | 0.9534 | 0.7951 |
| MEDICAL_SIMILAR_LEGAL_DIFFERENT (ì˜í•™ì  ìœ ì‚¬-ë²•ì  ìƒì´) | 0.9936 | 0.9434 | 0.9414 | 0.7812 |
| SYMPTOM_DISEASE (ì¦ìƒ-ì§ˆë³‘) | 0.9934 | 0.9690 | 0.9766 | 0.8500 |

Mejurix ëª¨ë¸ì€ íŠ¹íˆ MEDICAL_SIMILAR_LEGAL_DIFFERENT(0.9936)ì™€ SEVERITY_COMPENSATION(0.9997)ê³¼ ê°™ì€ í¬ë¡œìŠ¤ ë„ë©”ì¸ ê´€ê³„ì—ì„œ íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ì´ëŸ¬í•œ ë³µì¡í•œ ê´€ê³„ ìœ í˜•ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ í° ê°œì„ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

![ê´€ê³„ ìœ í˜•ë³„ ëª¨ë¸ ë¹„êµ](./model_relationship_comparison.png)

## ëª¨ë¸ ì‚¬ìš© ë°©ë²•

ì´ ëª¨ë¸ì€ Hugging Face Hubì—ì„œ ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥í•˜ë©°, Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ íŠ¹ì„± ì¶”ì¶œ, ë¬¸ì¥ ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Transformersë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import torch
from transformers import AutoModel, AutoTokenizer

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "mejurix/medical-legal-embedder"  # Hugging Face Hubì— ìˆëŠ” ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±
text = "í™˜ìëŠ” L3 ì²™ì¶” ê³¨ì ˆ ì§„ë‹¨ì„ ë°›ì•˜ìœ¼ë©°, ë³´ìƒ ì²­êµ¬ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤."
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)

with torch.no_grad():
    outputs = model(**inputs)

# ë¬¸ì¥ í‘œí˜„ì— [CLS] í† í° ì„ë² ë”© ì‚¬ìš©
embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í°
print(f"ì„ë² ë”© í˜•íƒœ: {embeddings.shape}")  # [1, 256]ì´ì–´ì•¼ í•¨
```

### ìœ ì‚¬ë„ ê³„ì‚°ì— ëª¨ë¸ ì‚¬ìš©í•˜ê¸°

```python
import torch
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "mejurix/medical-legal-embedder"  # Hugging Face Hubì— ìˆëŠ” ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :]  # [CLS] í† í° ì„ë² ë”©

def compute_similarity(text1, text2):
    emb1 = get_embedding(text1)
    emb2 = get_embedding(text2)
    return F.cosine_similarity(emb1, emb2).item()

# ì˜ˆì‹œ
text1 = "L3 ì²™ì¶” ê³¨ì ˆ ì§„ë‹¨ì„ ë°›ì•˜ìŠµë‹ˆë‹¤."
text2 = "ìš”ì¶” ë¶€ìƒì— ëŒ€í•œ ë³´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤."
similarity = compute_similarity(text1, text2)
print(f"ìœ ì‚¬ë„: {similarity:.4f}")
```

### Hugging Face íŒŒì´í”„ë¼ì¸ ì‚¬ìš©í•˜ê¸°

```python
from transformers import pipeline

# íŠ¹ì„± ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ìƒì„±
extractor = pipeline(
    "feature-extraction",
    model="mejurix/medical-legal-embedder",  # Hugging Face Hubì— ìˆëŠ” ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ
    tokenizer="mejurix/medical-legal-embedder"
)

# íŠ¹ì„± ì¶”ì¶œ
text = "í™˜ìëŠ” ì²™ì¶” ìˆ˜ìˆ  í›„ ë¬¼ë¦¬ ì¹˜ë£Œê°€ í•„ìš”í•©ë‹ˆë‹¤."
features = extractor(text)

# ì¶œë ¥ì€ [1, sequence_length, hidden_size] í˜•íƒœì˜ ì¤‘ì²©ëœ ë¦¬ìŠ¤íŠ¸
```

## í™œìš© ë¶„ì•¼ ë° í•œê³„ì 

### í™œìš© ë¶„ì•¼

- ì˜ë£Œ-ë²•ë¥  ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„
- ì‚¬ë¡€ ê´€ë ¨ì„± í‰ê°€
- ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ë° ì¡°ì§í™”
- ì˜ë£Œ ë° ë²•ë¥  ë„ë©”ì¸ì—ì„œì˜ ì •ë³´ ê²€ìƒ‰
- ì˜ë£Œ ê¸°ë¡ê³¼ ë²•ì  ì„ ë¡€ì˜ ìƒí˜¸ ì°¸ì¡°
- ì‚¬ìš©ì ì •ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì‚¬ìš©í•œ ì œë¡œìƒ· í…ìŠ¤íŠ¸ ë¶„ë¥˜

### í•œê³„ì 

- ë¶€ì •ë¬¸ì— ëŒ€í•œ ì´í•´ ì œí•œ(í˜„ì¬ ìœ ì‚¬ë„: 0.7791)
- ì‹œê°„ì  ë§¥ë½ êµ¬ë¶„ ê°œì„  í•„ìš”
- ì˜ë£Œ ìƒíƒœì˜ ì¤‘ì¦ë„ ìˆ˜ì¤€ì„ ì™„ì „íˆ êµ¬ë¶„í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŒ
- ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ 512 í† í°(BERT ì•„í‚¤í…ì²˜ì—ì„œ ìƒì†)

## í•™ìŠµ ë° í‰ê°€

### í•™ìŠµ

ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ê´€ê³„ ìœ í˜•(ì§ˆë³‘-ì¹˜ë£Œ, ì¤‘ì¦ë„-ë³´ìƒ ë“±)ì„ í¬í•¨í•˜ëŠ” ì˜ë£Œ-ë²•ë¥  ë¬¸ì„œ ìŒì˜ íŠ¹ìˆ˜ ë°ì´í„°ì…‹ì—ì„œ ë¯¸ì„¸ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤. í•™ìŠµì—ëŠ” ì–´ë ¤ìš´ ë¶€ì •ì  ì‚¬ë¡€ ë§ˆì´ë‹ì„ í†µí•œ íŠ¸ë¦¬í”Œë › ì†ì‹¤(triplet loss)ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

**í•™ìŠµ êµ¬ì„±:**
- ê¸°ë°˜ ëª¨ë¸: medicalai/ClinicalBERT
- ì„ë² ë”© ì°¨ì› ì¶•ì†Œ: 768 â†’ 256
- ë“œë¡­ì•„ì›ƒ: 0.5
- í•™ìŠµë¥ : 1e-5
- ë°°ì¹˜ í¬ê¸°: 16
- ê°€ì¤‘ì¹˜ ê°ì†Œ: 0.1
- íŠ¸ë¦¬í”Œë › ë§ˆì§„: 2.0
- ì—í­: 15

## ì¸ìš©

í•™ìˆ  ì—°êµ¬ì—ì„œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ì¸ìš©í•´ ì£¼ì„¸ìš”:

```
@software{mejurix_medicallegal_embedder,
  author = {Mejurix},
  title = {Mejurix Medical-Legal Embedding Model},
  year = {2025},
  version = {0.1.0},
  url = {https://huggingface.co/mejurix/medical-legal-embedder}
}
```

## ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ì— ë”°ë¼ ë°°í¬ë©ë‹ˆë‹¤. 
